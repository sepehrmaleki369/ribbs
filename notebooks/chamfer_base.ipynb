{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while os.getcwd() !='/home/ri/Desktop/Projects/Codebase':\n",
    "    os.chdir('../')\n",
    "\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from core.general_dataset.base import GeneralizedDataset\n",
    "from core.general_dataset.collate import worker_init_fn\n",
    "from core.utils import yaml_read\n",
    "from models.base_models import UNet\n",
    "from metrics.apls import APLS\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Config paths & hyperparameters\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "DATASET_YAML = \"./configs/dataset/mass_test.yaml\"\n",
    "MODEL_YAML   = \"./configs/model/baseline.yaml\"\n",
    "CKPT_IN      = \"/home/ri/Desktop/Projects/Codebase/AllFayzad/sdf-first-run/epoch002999.ckpt\"\n",
    "\n",
    "NUM_EPOCHS   = 100\n",
    "LR           = 1e-4\n",
    "NUM_WORKERS  = 0\n",
    "CROP_H, CROP_W = 512, 512\n",
    "# CROP_H, CROP_W = 256, 256\n",
    "# CROP_H, CROP_W = 1024, 1024\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Helpers\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def make_cfg(cfg, split):\n",
    "    c = cfg.copy()\n",
    "    c[\"split\"] = split\n",
    "    if split in (\"valid\", \"test\"):\n",
    "        c[\"augmentations\"] = []\n",
    "    return c\n",
    "\n",
    "def center_crop(x: torch.Tensor, crop_h: int, crop_w: int) -> torch.Tensor:\n",
    "    _, _, H, W = x.shape\n",
    "    top  = (H - crop_h) // 2\n",
    "    left = (W - crop_w) // 2\n",
    "    return x[:, :, top:top+crop_h, left:left+crop_w]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Device\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Build dataset (valid split → no augmentations)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "raw_cfg = yaml_read(DATASET_YAML)\n",
    "fixed_ds = GeneralizedDataset(make_cfg(raw_cfg, \"valid\"))\n",
    "smoke_ds = Subset(fixed_ds, indices=[0, 1])\n",
    "smoke_dl = DataLoader(\n",
    "    smoke_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn,\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Model, loss, metric, optimizer\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_cfg = yaml_read(MODEL_YAML)[\"params\"]\n",
    "model     = UNet(**model_cfg).to(device)\n",
    "\n",
    "if os.path.isfile(CKPT_IN):\n",
    "    ckpt = torch.load(CKPT_IN, map_location=device)\n",
    "    state = ckpt.get(\"state_dict\", ckpt)\n",
    "    state = {k.replace(\"model.\", \"\"): v for k, v in state.items()}\n",
    "    state = {k: v for k, v in state.items() if k in model.state_dict()}\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    print(f\"Loaded {len(state)} tensors from checkpoint.\")\n",
    "\n",
    "apls_fn = APLS(\n",
    "    data_dim=2, threshold=0, angle_range=(135,225),\n",
    "    max_nodes=1000, max_snap_dist=4, allow_renaming=True,\n",
    "    min_path_length=10, greater_is_road=False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "batch = next(iter(smoke_dl))\n",
    "x = center_crop(batch[\"image_patch\"], CROP_H, CROP_W).to(device, non_blocking=True)\n",
    "y = center_crop(batch[\"sdf_patch\"],   CROP_H, CROP_W).to(device, non_blocking=True)\n",
    "\n",
    "pred = model(x)\n",
    "\n",
    "\n",
    "plt.imshow(x[0].detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(y[0,0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(pred[0,0].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================\n",
    "# Main Functions with Applied Considerations\n",
    "# ==============================================================\n",
    "\n",
    "def sample_pred_at_positions(pred, positions):\n",
    "    r = positions[:, 0]\n",
    "    c = positions[:, 1]\n",
    "    r0 = r.floor().long()\n",
    "    c0 = c.floor().long()\n",
    "    r1 = r0 + 1\n",
    "    c1 = c0 + 1\n",
    "    dr = (r - r0.float()).unsqueeze(1)\n",
    "    dc = (c - c0.float()).unsqueeze(1)\n",
    "    H, W = pred.shape\n",
    "    r0 = r0.clamp(0, H - 1)\n",
    "    r1 = r1.clamp(0, H - 1)\n",
    "    c0 = c0.clamp(0, W - 1)\n",
    "    c1 = c1.clamp(0, W - 1)\n",
    "    Ia = pred[r0, c0].unsqueeze(1)\n",
    "    Ib = pred[r0, c1].unsqueeze(1)\n",
    "    Ic = pred[r1, c0].unsqueeze(1)\n",
    "    Id = pred[r1, c1].unsqueeze(1)\n",
    "    val = (Ia * (1 - dr) * (1 - dc) +\n",
    "           Ib * (1 - dr) * dc +\n",
    "           Ic * dr * (1 - dc) +\n",
    "           Id * dr * dc)\n",
    "    return val.squeeze(1)\n",
    "\n",
    "def compute_normals(sdf):\n",
    "    H, W = sdf.shape\n",
    "    grad_row = torch.zeros_like(sdf)\n",
    "    grad_col = torch.zeros_like(sdf)\n",
    "    # blur = torchvision.transforms.GaussianBlur((3,3), sigma=(1,1))\n",
    "    # sdf_smoothed = blur(sdf[None])[0]\n",
    "    sdf_smoothed = sdf\n",
    "    grad_row[1:-1] = (sdf_smoothed[2:] - sdf_smoothed[:-2]) / 2.0\n",
    "    grad_col[:, 1:-1] = (sdf_smoothed[:, 2:] - sdf_smoothed[:, :-2]) / 2.0\n",
    "    grad_row[0]    = sdf_smoothed[1] - sdf_smoothed[0]\n",
    "    grad_row[-1]   = sdf_smoothed[-1] - sdf_smoothed[-2]\n",
    "    grad_col[:, 0] = sdf_smoothed[:, 1] - sdf_smoothed[:, 0]\n",
    "    grad_col[:, -1] = sdf_smoothed[:, -1] - sdf_smoothed[:, -2]\n",
    "    normals = torch.stack([grad_row, grad_col], dim=2)\n",
    "    return normals\n",
    "\n",
    "def sample_normals_at_positions(normals, positions, normalize=True):\n",
    "    H, W, _ = normals.shape\n",
    "    r = positions[:, 0]\n",
    "    c = positions[:, 1]\n",
    "    r0 = r.floor().long()\n",
    "    c0 = c.floor().long()\n",
    "    r1 = r0 + 1\n",
    "    c1 = c0 + 1\n",
    "    dr = (r - r0.float()).unsqueeze(1)\n",
    "    dc = (c - c0.float()).unsqueeze(1)\n",
    "    r0 = r0.clamp(0, H - 1)\n",
    "    r1 = r1.clamp(0, H - 1)\n",
    "    c0 = c0.clamp(0, W - 1)\n",
    "    c1 = c1.clamp(0, W - 1)\n",
    "    Ia = normals[r0, c0]\n",
    "    Ib = normals[r0, c1]\n",
    "    Ic = normals[r1, c0]\n",
    "    Id = normals[r1, c1]\n",
    "    normal_interp = (Ia * (1 - dr) * (1 - dc) +\n",
    "                     Ib * (1 - dr) * dc +\n",
    "                     Ic * dr * (1 - dc) +\n",
    "                     Id * dr * dc)\n",
    "    \n",
    "    if normalize:\n",
    "        norm_val = torch.norm(normal_interp, dim=1, keepdim=True) + 1e-8\n",
    "        normal_interp = normal_interp / norm_val\n",
    "\n",
    "    return normal_interp\n",
    "\n",
    "def extract_zero_crossings_interpolated_positions(sdf_tensor, requires_grad=False):\n",
    "    \"\"\"\n",
    "    Extract zero crossings for a specified direction.\n",
    "    \n",
    "    - 'horizontal': Compare vertical neighbors (changes along rows).\n",
    "    - 'vertical': Compare horizontal neighbors (changes along columns).\n",
    "    - 'main_diagonal': Compare (i, j) with (i+1, j+1).\n",
    "    - 'anti_diagonal': Compare (i, j) with (i+1, j-1).\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    positions = []\n",
    "    H, W = sdf_tensor.shape\n",
    "    sdf_np = sdf_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    # Compare vertical neighbors\n",
    "    for i in range(H - 1):\n",
    "        for j in range(W):\n",
    "            v1 = sdf_np[i, j]\n",
    "            v2 = sdf_np[i + 1, j]\n",
    "            if v1 == 0:\n",
    "                positions.append([i, j])\n",
    "            elif v2 == 0:\n",
    "                positions.append([i + 1, j])\n",
    "            elif v1 * v2 < 0:\n",
    "                # alpha = min(abs(v1), abs(v2)) / (abs(v1) + abs(v2) + epsilon)\n",
    "                # alpha = abs(v1) / abs(v2)\n",
    "                alpha = abs(v1) / (abs(v1) + abs(v2) + epsilon)\n",
    "                row_interp = i + alpha\n",
    "                positions.append([row_interp, j])\n",
    "    # Compare horizontal neighbors\n",
    "    for i in range(H):\n",
    "        for j in range(W - 1):\n",
    "            v1 = sdf_np[i, j]\n",
    "            v2 = sdf_np[i, j + 1]\n",
    "            if v1 == 0:\n",
    "                positions.append([i, j])\n",
    "            elif v2 == 0:\n",
    "                positions.append([i, j + 1])\n",
    "            elif v1 * v2 < 0:\n",
    "                # alpha = min(abs(v1), abs(v2)) / (abs(v1) + abs(v2) + epsilon)\n",
    "                # alpha = abs(v1) / abs(v2)\n",
    "                alpha = abs(v1) / (abs(v1) + abs(v2) + epsilon)\n",
    "\n",
    "                col_interp = j + alpha\n",
    "                positions.append([i, col_interp])\n",
    "                    \n",
    "    if positions:\n",
    "        return torch.tensor(positions, dtype=torch.float32, device=sdf_tensor.device, requires_grad=requires_grad)\n",
    "    else:\n",
    "        return torch.empty((0, 2), dtype=torch.float32, device=sdf_tensor.device, requires_grad=requires_grad)\n",
    "    \n",
    "def compute_chamfer_distance(points1, points2):\n",
    "    if points1.numel() == 0 or points2.numel() == 0:\n",
    "        return torch.tensor(float('inf'), device=points1.device)\n",
    "    diff = points1.unsqueeze(1) - points2.unsqueeze(0)\n",
    "    dists = torch.norm(diff, dim=2)\n",
    "    min_dists1, _ = torch.min(dists, dim=1)\n",
    "    min_dists2, _ = torch.min(dists, dim=0)\n",
    "    return -torch.mean(min_dists1) + torch.mean(min_dists2)\n",
    "\n",
    "\n",
    "def manual_chamfer_grad(pred_sdf, pred_zc, gt_zc, update_scale=1.0, dist_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Compute a 'manual' gradient for Chamfer-like boundary alignment.\n",
    "    We ignore any predicted zero crossings that are too far from\n",
    "    all ground-truth zero crossings, i.e., they are considered spurious.\n",
    "    \n",
    "    Args:\n",
    "        pred_sdf (Tensor): [H, W], the predicted SDF.\n",
    "        pred_zc  (Tensor): [N, 2], predicted zero-crossing positions (row, col).\n",
    "        gt_zc    (Tensor): [M, 2], ground-truth zero-crossing positions (row, col).\n",
    "        update_scale (float): scaling factor for the gradient magnitude.\n",
    "        dist_threshold (float): maximum distance for a predicted ZC to be considered valid.\n",
    "    \n",
    "    Returns:\n",
    "        dSDF (Tensor): [H, W], the gradient w.r.t. pred_sdf for the Chamfer loss.\n",
    "    \"\"\"\n",
    "    # Initialize gradient buffer\n",
    "    dSDF = torch.zeros_like(pred_sdf)\n",
    "    \n",
    "    # Compute normals on the entire SDF\n",
    "    normals = compute_normals(pred_sdf)  # shape: [H, W, 2]\n",
    "    sampled_normals = sample_normals_at_positions(normals, pred_zc)  # shape: [N, 2]\n",
    "    \n",
    "    # Move data to CPU for distance computations\n",
    "    gt_zc_cpu = gt_zc.detach().cpu()\n",
    "    pred_zc_cpu = pred_zc.detach().cpu()\n",
    "\n",
    "    # Loop over each predicted zero crossing\n",
    "    for i in range(pred_zc.shape[0]):\n",
    "        # p is (row, col)\n",
    "        p = pred_zc_cpu[i]\n",
    "\n",
    "        # Find nearest ground-truth crossing\n",
    "        diff = gt_zc_cpu - p  # shape: [M, 2]\n",
    "        dist = torch.norm(diff, dim=1)  # [M]\n",
    "        min_dist, min_index = torch.min(dist, dim=0)\n",
    "\n",
    "        # If the predicted ZC is too far from every GT ZC, skip it (spurious)\n",
    "        if min_dist > dist_threshold:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, compute the usual chamfer update\n",
    "        matched_gt = gt_zc_cpu[min_index]\n",
    "        dl_dp = matched_gt - p  # direction from predicted ZC to GT ZC\n",
    "\n",
    "        # Project onto local normal at p\n",
    "        n = sampled_normals[i]\n",
    "        n_norm = torch.norm(n) + 1e-8\n",
    "        n = n / n_norm\n",
    "        dot_val = torch.dot(dl_dp.to(n.device), n) * update_scale\n",
    "\n",
    "        # Distribute the gradient bilinearly among neighboring pixels\n",
    "        r, c = p[0].item(), p[1].item()\n",
    "        r0, c0 = int(np.floor(r)), int(np.floor(c))\n",
    "        r1, c1 = r0 + 1, c0 + 1\n",
    "        w_r1 = r - r0\n",
    "        w_r0 = 1 - w_r1\n",
    "        w_c1 = c - c0\n",
    "        w_c0 = 1 - w_c1\n",
    "\n",
    "        H, W = dSDF.shape\n",
    "        if 0 <= r0 < H and 0 <= c0 < W:\n",
    "            dSDF[r0, c0] += dot_val * w_r0 * w_c0\n",
    "        if 0 <= r0 < H and 0 <= c1 < W:\n",
    "            dSDF[r0, c1] += dot_val * w_r0 * w_c1\n",
    "        if 0 <= r1 < H and 0 <= c0 < W:\n",
    "            dSDF[r1, c0] += dot_val * w_r1 * w_c0\n",
    "        if 0 <= r1 < H and 0 <= c1 < W:\n",
    "            dSDF[r1, c1] += dot_val * w_r1 * w_c1\n",
    "\n",
    "    return dSDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "NUM_EPOCHS   = 100\n",
    "λ_chamfer    = 1.0\n",
    "λ_sdf        = 1.0\n",
    "update_scale = 1\n",
    "dist_threshold = 1.5\n",
    "\n",
    "# Buffers for plotting over time\n",
    "epoch_losses       = []\n",
    "mean_grad_mags     = []\n",
    "\n",
    "def update_pred_sdf(gt_sdf, pred_sdf, gt_zc, pred_zc, optimizer, iter_num, update_scale=1.0, dist_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Updated function that uses a MeshSDF-like trick with bilinear gradient distribution for the Chamfer term,\n",
    "    combined with a pixel-wise SDF loss.\n",
    "    \"\"\"\n",
    "    pred_sdf.requires_grad_(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    dSDF_chamfer = manual_chamfer_grad(pred_sdf, pred_zc, gt_zc, update_scale=update_scale, dist_threshold=dist_threshold)\n",
    "    with torch.no_grad():\n",
    "        pred_sdf.grad = dSDF_chamfer.clone()\n",
    "\n",
    "    # pred_zc_long = pred_zc.long()\n",
    "    # print('pred_zc_long.shape:', pred_zc_long.shape)\n",
    "    # rows, cols = pred_zc_long[:, 0], pred_zc_long[:, 1]\n",
    "    # loss_pred = pred_sdf[rows, cols].sum()\n",
    "    zc_values = sample_pred_at_positions(pred_sdf, pred_zc)\n",
    "    loss_pred = (zc_values).sum()\n",
    "    loss_pred.backward(retain_graph=True)\n",
    "    # loss_pred = torch.abs(zc_values).mean()\n",
    "    # loss_pred.backward(retain_graph=True)\n",
    "    # loss_pred = (torch.abs(zc_values)).sum()\n",
    "    # loss_pred.backward(retain_graph=True)\n",
    "    # print(f\"[Iter {iter_num}] Pixel-Value Loss: {loss_pred.item():.4f}\")\n",
    "    \n",
    "    normals = compute_normals(pred_sdf)\n",
    "    sampled_normals = sample_normals_at_positions(normals, pred_zc)\n",
    "    dl_dx_fake = torch.zeros_like(pred_zc)\n",
    "    dl_ds_per_point = - torch.sum(sampled_normals * dl_dx_fake, dim=1)\n",
    "    combined_loss = torch.sum(dl_ds_per_point)\n",
    "    combined_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return pred_sdf\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in smoke_dl:\n",
    "        # 1) Prep data\n",
    "        x = center_crop(batch[\"image_patch\"], CROP_H, CROP_W).to(device, non_blocking=True)\n",
    "        y = center_crop(batch[\"sdf_patch\"],   CROP_H, CROP_W).to(device, non_blocking=True)\n",
    "\n",
    "        # 2) Forward\n",
    "        pred_sdf_b = model(x).squeeze(1)  # [B,H,W]\n",
    "        gt_sdf_b   = y.squeeze(1)         # [B,H,W]\n",
    "\n",
    "        # 3) Compute per-sample losses\n",
    "        batch_loss = 0.0\n",
    "        for b in range(pred_sdf_b.size(0)):\n",
    "            p = pred_sdf_b[b]\n",
    "            g = gt_sdf_b[b]\n",
    "\n",
    "            pred_zc = extract_zero_crossings_interpolated_positions(p)\n",
    "            gt_zc   = extract_zero_crossings_interpolated_positions(g)\n",
    "\n",
    "            loss_chamfer = torch.abs(compute_chamfer_distance(pred_zc, gt_zc))\n",
    "            loss_sdf     = F.l1_loss(p, g)\n",
    "\n",
    "            batch_loss += λ_chamfer * loss_chamfer + λ_sdf * loss_sdf\n",
    "\n",
    "        batch_loss = batch_loss / pred_sdf_b.size(0)\n",
    "\n",
    "        # 4) Backprop + step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(smoke_dl)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    # — compute mean |dSDF| on the first sample for monitoring —\n",
    "    with torch.no_grad():\n",
    "        sample_p = pred_sdf_b[0].detach().requires_grad_(True)\n",
    "        sample_g = gt_sdf_b[0].detach()\n",
    "        p_zc = extract_zero_crossings_interpolated_positions(sample_p)\n",
    "        g_zc = extract_zero_crossings_interpolated_positions(sample_g)\n",
    "\n",
    "        dSDF = manual_chamfer_grad(\n",
    "            sample_p, p_zc, g_zc,\n",
    "            update_scale=update_scale,\n",
    "            dist_threshold=dist_threshold\n",
    "        )\n",
    "        mean_grad_mags.append(dSDF.abs().mean().item())\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}/{NUM_EPOCHS:03d}]  Loss: {avg_loss:.4f}  |dSDF|ₘₑₐₙ: {mean_grad_mags[-1]:.4f}\")\n",
    "\n",
    "    # —––––—–––– VISUALIZE EVERYTHING —––––—––––\n",
    "    # Convert to CPU/numpy\n",
    "    sample_img  = x[0].cpu().permute(1,2,0).numpy()\n",
    "    sample_gt   = sample_g.cpu().numpy()\n",
    "    sample_pred = sample_p.detach().cpu().numpy()\n",
    "    p_zc_np     = p_zc.cpu().numpy()\n",
    "    g_zc_np     = g_zc.cpu().numpy()\n",
    "    dSDF_map    = dSDF.detach().cpu().numpy()\n",
    "\n",
    "    # set a symmetric vmin/vmax for the SDF plots\n",
    "    mx = max(sample_gt.max(), sample_gt.min()*-1,\n",
    "             sample_pred.max(), sample_pred.min()*-1)\n",
    "    vmin, vmax = -mx, mx\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12,10))\n",
    "    fig.suptitle(f\"Epoch {epoch}\", fontsize=16)\n",
    "\n",
    "    # GT SDF + ZCs\n",
    "    axs[0,0].imshow(sample_gt, cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axs[0,0].scatter(g_zc_np[:,1], g_zc_np[:,0], s=8, c='lime')\n",
    "    axs[0,0].set_title(\"GT SDF + ZCs\")\n",
    "    axs[0,0].axis('off')\n",
    "\n",
    "    # Predicted SDF + ZCs\n",
    "    axs[0,1].imshow(sample_pred, cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axs[0,1].scatter(p_zc_np[:,1], p_zc_np[:,0], s=8, c='red')\n",
    "    axs[0,1].set_title(\"Pred SDF + ZCs\")\n",
    "    axs[0,1].axis('off')\n",
    "\n",
    "    # Chamfer gradient heatmap\n",
    "    im = axs[1,0].imshow(dSDF_map, cmap='viridis')\n",
    "    axs[1,0].set_title(\"Chamfer ∂Loss/∂SDF\")\n",
    "    axs[1,0].axis('off')\n",
    "    fig.colorbar(im, ax=axs[1,0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Loss & mean grad‐mag curve\n",
    "    axs[1,1].plot(range(1, epoch+1), epoch_losses, label=\"Loss\")\n",
    "    axs[1,1].plot(range(1, epoch+1), mean_grad_mags, label=\"Mean |dSDF|\")\n",
    "    axs[1,1].set_title(\"Loss & mean |dSDF| over epochs\")\n",
    "    axs[1,1].set_xlabel(\"Epoch\")\n",
    "    axs[1,1].legend()\n",
    "    axs[1,1].grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eeb489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Hyper‑parameters ────────────────────────────────────────────\n",
    "NUM_EPOCHS     = 100\n",
    "LR             = 1e-4\n",
    "\n",
    "# ─── Model, optimizer, loss ──────────────────────────────────────\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "from losses.chamfer_loss import (\n",
    "    SDFChamferLoss,\n",
    ")\n",
    "loss_fn = SDFChamferLoss(weight_sdf=1, band=0, reduction='sum', update_scale=1, normalize_normals=True, iso=0)\n",
    "\n",
    "\n",
    "# ─── Training loop ───────────────────────────────────────────────\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in smoke_dl:\n",
    "        x = center_crop(batch[\"image_patch\"], CROP_H, CROP_W).to(device)\n",
    "        y = center_crop(batch[\"sdf_patch\"],   CROP_H, CROP_W).to(device)\n",
    "\n",
    "        pred = model(x)       # [B,1,H,W]\n",
    "        gt   = y               # [B,1,H,W]\n",
    "\n",
    "        # forward + loss\n",
    "        loss = loss_fn(pred, gt)\n",
    "\n",
    "        # backward + step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}/{NUM_EPOCHS:03d}]  Loss: {running_loss:.4f}\")\n",
    "\n",
    "    # ─── Visualization ───────────────────────────────────────────\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_img  = x[0].cpu().permute(1,2,0).numpy()\n",
    "        sample_gt   = gt[0,0].cpu().numpy()\n",
    "        sample_pred = pred[0,0].cpu().numpy()\n",
    "\n",
    "        \n",
    "        fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "        axes[0].imshow(sample_gt, cmap='RdBu')\n",
    "        axes[0].set_title(\"GT SDF + ZCs\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(sample_pred, cmap='RdBu')\n",
    "        axes[1].set_title(\"Pred SDF + ZCs\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Epoch {epoch}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"apply_final_relu =\", model.apply_final_relu)   # must be False for SDFs\n",
    "print(\"pred range:\", pred.min().item(), pred.max().item())\n",
    "from losses.chamfer_loss import _zero_crossings_lin_interp\n",
    "P_all, P_cnt = _zero_crossings_lin_interp(pred.detach().squeeze(1))\n",
    "G_all, G_cnt = _zero_crossings_lin_interp(gt.detach().squeeze(1))\n",
    "print(\"pred ZC per sample:\", P_cnt.tolist())\n",
    "print(\"gt   ZC per sample:\", G_cnt.tolist())\n",
    "\n",
    "print(f\"loss (scientific) = {loss.item():.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4929f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt.min().item(), gt.max().item())\n",
    "print((gt==0).sum())     # or torch.count_nonzero(torch.isclose(gt, torch.tensor(0.0), atol=1e-4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
